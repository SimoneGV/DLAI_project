{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from sentence_transformers.quantization import quantize_embeddings\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import re\n",
    "from embeddings_generator import generate_embeddings, text_from_embeddings\n",
    "from quantization_dequantization import quantization,dequantization\n",
    "from evaluation import evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to reduce the text dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    #Remove all punctuation characters from the text.\n",
    "    new_text = []\n",
    "    for char in text:\n",
    "        if char not in string.punctuation:\n",
    "            new_text.append(char)\n",
    "    return ''.join(new_text)\n",
    "\n",
    "def from_letter_to_number(text):\n",
    "    # Transform part of sentence into numbers\n",
    "\n",
    "    # Create the regex patterns\n",
    "    patterns = {\n",
    "        r'one': '1',\n",
    "        r'One': '1',\n",
    "        r'to': '2',\n",
    "        r'To': '2',\n",
    "        r'two': '2',\n",
    "        r'Two': '2',\n",
    "        r'tree': '3',\n",
    "        r'Tree': '3',\n",
    "        r'for': '4',\n",
    "        r'For': '4',\n",
    "        r'ate': '8',\n",
    "        r'Ate': '8',\n",
    "        r'height': '8',\n",
    "        r'Height': '8',\n",
    "    }\n",
    "    # Apply the patterns\n",
    "    for pattern, replacement in patterns.items():\n",
    "        text = re.sub(pattern, replacement, text)\n",
    "    return text\n",
    "\n",
    "def remove_double_consonants(text):\n",
    "    #Remove duplicate consecutive consonants from the text.\n",
    "    result = []\n",
    "    for i in range(len(text)):\n",
    "        # Check if the current character is a consonant and if it is equal to the previous character\n",
    "        if i > 0 and text[i].lower() == text[i-1].lower() and text[i].lower() not in 'aeiou':\n",
    "            continue\n",
    "        result.append(text[i])\n",
    "    return ''.join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the list of texts to use for lossy text compression\n",
    "(Feel free to use your personal text to test the system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Jack Morris is a PhD student at Cornell Tech in New York City\",\n",
    "    \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity\",\n",
    "    # General Knowledge\n",
    "    \"The Eiffel Tower is one of the most famous landmarks in Paris, France.\",\n",
    "    \"Albert Einstein developed the theory of relativity, fundamentally altering the understanding of physics.\",\n",
    "    # Literary Excerpt\n",
    "    \"In the beginning, God created the heavens and the earth.\",\n",
    "    \"To be, or not to be, that is the question.\",\n",
    "    # Dialogue\n",
    "    \"Can you please pass the salt?\",\n",
    "    \"Sure, Iâ€™ll meet you at the coffee shop at 3 PM.\",\n",
    "    # Technical/Scientific\n",
    "    \"Quantum entanglement describes a physical phenomenon where particles become interconnected and instantaneously affect each other, regardless of distance.\",\n",
    "    \"Photosynthesis is the process by which green plants convert sunlight into chemical energy.\",\n",
    "    # Questions\n",
    "    \"What is the capital of Japan?\",\n",
    "    \"How many continents are there on Earth?\",\n",
    "    # Short Sentences\n",
    "    \"The sun rises in the east.\",\n",
    "    \"Water is essential for life.\",\n",
    "    # Complex Sentence\n",
    "    \"Despite the challenges posed by the rugged terrain, the explorers pressed on, determined to reach the summit before the storm hit.\",\n",
    "    \"The conference on artificial intelligence attracted experts from various fields, sparking debates on the ethical implications of autonomous systems.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the size of the original list of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_texts_size = sum(len(text.encode('utf-8')) for text in texts)\n",
    "print(\"Size of original texts (in bytes):\", original_texts_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the text reduction functions and print out the new size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_no_punct = [remove_punctuation(text) for text in texts]\n",
    "print(texts_no_punct)\n",
    "\n",
    "texts_no_punct_size = sum(len(text.encode('utf-8')) for text in texts_no_punct)\n",
    "print(\"Size of texts with punctuation removed (in bytes):\", texts_no_punct_size)\n",
    "\n",
    "texts_lett_num = [from_letter_to_number(text) for text in texts_no_punct]\n",
    "print(texts_lett_num)\n",
    "\n",
    "texts_lett_num_size = sum(len(text.encode('utf-8')) for text in texts_lett_num)\n",
    "print(\"Size of texts with numbers (in bytes):\", texts_lett_num_size)\n",
    "\n",
    "texts_no_doubles = [remove_double_consonants(text) for text in texts_lett_num]\n",
    "print(texts_no_doubles)\n",
    "\n",
    "texts_no_doubles_size = sum(len(text.encode('utf-8')) for text in texts_no_doubles)\n",
    "print(\"Size of texts after removing double consonants (in bytes):\", texts_no_doubles_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate and quantize the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings\n",
    "embeddings = generate_embeddings(texts_no_doubles)\n",
    "\n",
    "print(\"Original embeddings:\")\n",
    "print(embeddings)\n",
    "print(\"Size in bytes:\", embeddings.nbytes)\n",
    "\n",
    "# Quantize embeddings to int8\n",
    "quantized_embeddings = quantization(embeddings)\n",
    "print(\"Quantized embeddings:\")\n",
    "print(quantized_embeddings)\n",
    "print(\"Size in bytes:\", quantized_embeddings.nbytes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dequantize the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dequantized_embeddings = dequantization(quantized_embeddings)\n",
    "print(\"Dequantized embeddings:\")\n",
    "print(dequantized_embeddings)\n",
    "\n",
    "# Transform embeddings back into text\n",
    "dequantized_text = text_from_embeddings(dequantized_embeddings)\n",
    "print(\"Text from dequantized embeddings:\")\n",
    "print(dequantized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the quality of the text reconstruction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim, euclidean, bleu, avg_bleu, rouge, edit_dist, jaccard = evaluation(\n",
    "    embeddings, dequantized_embeddings, texts_no_doubles, dequantized_text\n",
    ")\n",
    "\n",
    "print(f\"Cosine Similarity between original and dequantized embeddings: {cos_sim:.4f}\")\n",
    "print(f\"Euclidean Distance between original and dequantized embeddings: {euclidean:.4f}\")\n",
    "print(f\"BLEU score between original and dequantized texts: {bleu}\")\n",
    "print(f\"Average BLEU: {avg_bleu:.4f}\")\n",
    "print(f\"ROUGE-1: {rouge['rouge1'].mid}\")\n",
    "print(f\"ROUGE-2: {rouge['rouge2'].mid}\")\n",
    "print(f\"ROUGE-L: {rouge['rougeL'].mid}\")\n",
    "print(f\"Edit distance between texts and dequantized texts: {edit_dist}\")\n",
    "print(f\"Jaccard distance between texts and dequantized texts: {jaccard}\")\n",
    "print(f\"Average Jaccard distance between texts and dequantized texts: {np.mean(jaccard)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
